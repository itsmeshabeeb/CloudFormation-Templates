AWSTemplateFormatVersion: '2010-09-09'
Description: Convert CloudFront access log to JSON via Firehose (20181130)
Parameters:
  S3Expiredate:
    Description: Number of days to keep S3 file (S3 TTL)
    Type: String
    Default: 10
  CWlogsExpiredate:
    Description: Number of days to keep Cloudwatch logs (logs TTL)
    Type: String
    Default: 3
  S3AthenaExpiredate:
    Description: Number of days to keep S3 file (Athena)
    Type: String
    Default: 45
  FirehoseIntervalInSeconds:
    Description: Firehose output interval
    Type: String
    Default: 900
Resources:
  Step1S3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      LifecycleConfiguration:
        Rules:
          - Id: AutoDelete
            Status: Enabled
            ExpirationInDays: !Ref 'S3Expiredate'
      NotificationConfiguration:
        TopicConfigurations:
          - Event: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: gz
            Topic: !Ref 'Step2SnsTopic'
      VersioningConfiguration:
        Status: Enabled

  Step2SnsTopicPolicy:
    Type: AWS::SNS::TopicPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Id: MyTopicPolicy
        Statement:
          - Sid: allow-publish-s3
            Effect: Allow
            Principal:
              Service:
                - s3.amazonaws.com
            Action:
              - sns:Publish
            Resource: !Ref 'Step2SnsTopic'
      Topics:
        - !Ref 'Step2SnsTopic'

  Step2SnsTopic:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: !Sub 's3-logs-ObjectCreated'
      Subscription:
        - Endpoint: !GetAtt 'Step3LambdaFunction.Arn'
          Protocol: lambda

  Step3LambdaLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref 'Step3LambdaFunction'
      Action: lambda:InvokeFunction
      Principal: sns.amazonaws.com
      SourceArn: !Ref 'Step2SnsTopic'

  Step3LogGroupLambda:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${AWS::StackName}/${Step3LambdaFunction}'
      RetentionInDays: !Ref 'CWlogsExpiredate'

  Step3LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource: !Sub 'arn:aws:s3:::${AWS::StackName}-*'
              - Effect: Allow
                Action:
                  - firehose:PutRecordBatch
                Resource: !GetAtt 'Step4deliverystream.Arn'
              - Effect: Allow
                Action:
                  - sqs:SendMessage
                Resource: !GetAtt [Step3SqsDeadLetterQueue, Arn]

  Step3LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt 'Step3LambdaRole.Arn'
      DeadLetterConfig: 
        TargetArn: !GetAtt [Step3SqsDeadLetterQueue, Arn]
      Code:
        ZipFile: !Sub |
          import boto3
          import json
          import os
          import urllib.parse
          import gzip
          from datetime import datetime
          import base64
          import re

          s3 = boto3.client('s3')
          firehose = boto3.client('firehose')

          def lambda_handler(event, context):
            a = parse_s3_event(event)
            bucket_name = a['bucket_name']
            key = a['key']

            # Process CloudFront log (.gz)
            if (re.match('.*.gz$', key)):
              response =s3.get_object(Bucket=bucket_name, Key=key)
              body = gzip.decompress(response['Body'].read()).decode('utf-8','ignore').splitlines()
              if len(body) > 0:
                process_log(body)
        
          def parse_s3_event(event):
            a = json.loads(event['Records'][0]['Sns']['Message'])
            z = {}
            z['bucket_name'] = a['Records'][0]['s3']['bucket']['name']
            z['key'] = urllib.parse.unquote_plus(a['Records'][0]['s3']['object']['key'], encoding='utf-8')
            return z

          def process_log(data):
            i = 0
            c = []
            for a in data:
              b = parse_log(a)
              if b is not None:
                c.append({'Data': b})
                i = i + 1
              if i == 100:
                PutRecordBatchFirehose(c)
                i = 0
                c = []
            if len(c) > 0:
              PutRecordBatchFirehose(c)

          def parse_log(line):
            z = {}
            a = line.split('\t')

            # cloudfront Log
            if len(a) > 25:
              if (re.match('[0-9]...-[0-9].-[0-9].' , a[0])):
                if (re.match('[0-9].:[0-9].:[0-9].' , a[1])):
                  z = parse_cloudfront_log(a)

            #Column check (number)
            if len(z) > 25:
              #print(z)
              return json.dumps(z) + "\n"

          def parse_cloudfront_log(b):
            z = {}
            z["date"] = b[0]
            z["time"] = b[1]
            z["x_edge_location"] = b[2]
            z["sc_bytes"] = float(b[3])
            z["c_ip"] = b[4]
            z["cs_method"] = b[5]
            z["cs_host"] = b[6]
            z["cs_uri_stem"] = b[7]
            z["cs_status"] = b[8]
            z["cs_referer"] = b[9]
            z["cs_user_agent"] = b[10]
            z["cs_uri_query"] = b[11]
            z["cs_cookie"] = b[12]
            z["x_edge_result_type"] = b[13]
            z["x_edge_request_id"] = b[14]
            z["x_host_header"] = b[15]
            z["cs_protocol"] = b[16]
            z["cs_bytes"] = float(b[17])
            z["time_taken"] = float(b[18])
            z["x_forwarded_for"] = b[19]
            z["ssl_protocol"] = b[20]
            z["ssl_cipher"] = b[21]
            z["x_edge_response_result_type"] = b[22]
            z["cs_protocol_version"] = b[23]
            z["fle_status"] = b[3]
            z["fle_encrypted_fields"] = b[3]

            z["timestamp"] = z["date"] + 'T' + z["time"] + 'Z'

            return z

          def PutRecordBatchFirehose(data):
            firehose_stream_name = os.environ['firehose_stream_name']
            r = firehose.put_record_batch(
              DeliveryStreamName = firehose_stream_name,
              Records = data
            )

            #print(str(data))
            #print(str(r["ResponseMetadata"]["HTTPHeaders"]))

      Runtime: python3.6
      MemorySize: 256
      Timeout: 300
      Description: CloudFront accesslog S3 to firehose
      Environment:
        Variables:
          firehose_stream_name: !Ref 'Step4deliverystream'
      Tags:
        - Key: CloudformationArn
          Value: !Ref 'AWS::StackId'

  Step3SqsDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      MessageRetentionPeriod: 1209600

  Step4deliverystream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      ExtendedS3DestinationConfiguration:
        BucketARN: !Sub 'arn:aws:s3:::${Step5S3Bucket}'
        BufferingHints:
          IntervalInSeconds: !Ref 'FirehoseIntervalInSeconds'
          SizeInMBs: '100'
        CompressionFormat: GZIP
        Prefix: !Sub 'firehose/${AWS::StackName}/'
        RoleARN: !GetAtt 'Step4deliveryRole.Arn'
        ProcessingConfiguration:
          Enabled: 'false'

  Step4deliveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                sts:ExternalId: !Ref 'AWS::AccountId'

  Step4deliveryPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: firehose_delivery_policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - s3:AbortMultipartUpload
              - s3:GetBucketLocation
              - s3:GetObject
              - s3:ListBucket
              - s3:ListBucketMultipartUploads
              - s3:PutObject
            Resource:
              - !Sub 'arn:aws:s3:::${Step5S3Bucket}'
              - !Sub 'arn:aws:s3:::${Step5S3Bucket}*'
      Roles:
        - !Ref 'Step4deliveryRole'

  Step4LogGroupFirehose:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/firehose/${AWS::StackName}/${Step4deliverystream}'
      RetentionInDays: !Ref 'CWlogsExpiredate'

  Step5S3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      LifecycleConfiguration:
        Rules:
          - Id: AutoDelete
            Status: Enabled
            ExpirationInDays: !Ref 'S3Expiredate'
      NotificationConfiguration:
        TopicConfigurations:
          - Event: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .gz
                  - Name: prefix
                    Value: !Sub 'firehose/${AWS::StackName}/'
            Topic: !Ref 'Step6SnsTopic'
      VersioningConfiguration:
        Status: Enabled

  Step6SnsTopicPolicy:
    Type: AWS::SNS::TopicPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Id: MyTopicPolicy
        Statement:
        - Sid: allow-publish-s3
          Effect: Allow
          Principal:
            Service:
            - s3.amazonaws.com
          Action:
          - sns:Publish
          Resource: !Ref 'Step6SnsTopic'
      Topics:
      - !Ref 'Step6SnsTopic'

  Step6SnsTopic:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: s3-trigger-firehose-output
      Subscription:
      - Endpoint: !GetAtt 'Step7LambdaFunction.Arn'
        Protocol: lambda

  Step7LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref 'Step7LambdaFunction'
      Action: lambda:InvokeFunction
      Principal: sns.amazonaws.com
      SourceArn: !Ref 'Step6SnsTopic'

  Step7LogGroupLambda:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${AWS::StackName}/${Step7LambdaFunction}'
      RetentionInDays: 14

  Step7LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource: !Sub 'arn:aws:s3:::${AWS::StackName}-*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub 'arn:aws:s3:::${AWS::StackName}-*'
              - Effect: Allow
                Action:
                  - sqs:SendMessage
                Resource: !GetAtt [Step7SqsDeadLetterQueue, Arn]

  Step7SqsDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      MessageRetentionPeriod: 1209600

  Step7LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      DeadLetterConfig: 
        TargetArn: !GetAtt [Step7SqsDeadLetterQueue, Arn]
      Role: !GetAtt 'Step7LambdaRole.Arn'
      Code:
        ZipFile: !Sub |
          import boto3
          import json
          import os
          import gzip
          import shutil
          import urllib.parse
          def lambda_handler(event, context):

            z = parse_s3_event(event)
            bucket_name = z['bucket_name']
            key = z['key']

            data = download_file_s3(bucket_name,key)
            bucket_name2 = os.environ['s3_bucket']

            l_host = get_uniq_host(data)
            l_date = get_uniq_date(data)

            for h in l_host:
              for d in l_date:
                data2 = get_log_by_host(data,h,d)
                if len(data2) > 0:
                  key2 = get_key_with_partition(key,h,d)
                  upload_file_s3(data2,bucket_name2,key2)

          def parse_s3_event(event):
            #print('event: ' + str(event))
            a = json.loads(event['Records'][0]['Sns']['Message'])
            z = {}
            z['bucket_name'] = a['Records'][0]['s3']['bucket']['name']
            z['key'] = urllib.parse.unquote_plus(a['Records'][0]['s3']['object']['key'], encoding='utf-8')
            return z

          def download_file_s3(bucket,key):
            s3 = boto3.client('s3')
            s3.download_file(bucket, key, '/tmp/in-file.gz')
            c = []
            with gzip.open('/tmp/in-file.gz', 'rt') as a:
              for b in a:
                c.append(json.loads(b))
            return c

          def upload_file_s3(data,bucket,key):

            with open("/tmp/jsonl.tmp", "w", encoding="utf-8") as a:
              for b in data:
                a.write(json.dumps(b)+"\n")

            with open('/tmp/jsonl.tmp', 'rb') as c:
              with gzip.open('/tmp/jsonl.gz', 'wb', compresslevel=9) as d:
                shutil.copyfileobj(c, d)

            s3_client = boto3.client('s3')
            s3_client.upload_file('/tmp/jsonl.gz', bucket , key )

            # print('bucket:' + bucket)
            # print('key:' + key)

          def get_uniq_host(data):
            b = []
            for a in data:
              if 'x_host_header' in a.keys():
                b.append(a['x_host_header'])
            c = list(set(b))
            return c

          def get_uniq_date(data):
            b = []
            for a in data:
              if 'date' in a.keys():
                b.append(a['date'])
            c = list(set(b))
            return c

          def get_log_by_host(data,host,date):
            b = []
            for a in data:
              if 'date' in a.keys():
                if 'x_host_header' in a.keys():
                  if (a['date'] == date):
                    if (a['x_host_header'] == host):
                      b.append(a)
            return b

          def get_key_with_partition(key,host,date):
            a = key.split('/')
            a.reverse()
            b = date.split('-')
            z = {}
            z['filename'] = a[0]
            z['hour'] = a[1]
            z['day'] = b[2]
            z['month'] = b[1]
            z['year'] = b[0]
            f = 'Step7/host=' + host + '/year=' + z['year'] + '/month=' + z['month'] + '/day=' + z['day'] + '/' + z['filename']
            return f

      Runtime: python3.6
      MemorySize: 256
      Timeout: 600
      Description: Copy the S3 file output by Firehose for Athena (with partition)
      Environment:
        Variables:
          CfnStackName: !Sub '${AWS::StackName}'
          s3_bucket: !Ref 'Step8S3Bucket'
      Tags:
        - Key: CloudformationArn
          Value: !Ref 'AWS::StackId'

  Step8S3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      LifecycleConfiguration:
        Rules:
          - Id: AutoDelete
            Status: Enabled
            ExpirationInDays: !Ref 'S3AthenaExpiredate'

  Step9GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref 'AWS::AccountId'
      DatabaseInput:
        Name: !Sub 'cloudfront'
        Description: Glue Database for ETL processing by Athena
  Step9GlueRole:
    Type: AWS::IAM::Role
    Properties:
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:*
                Resource: !Sub 'arn:aws:s3:::${AWS::StackName}-*'

  Step9GlueCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub 'crawler_for_cloudfrontlogs'
      Role: !GetAtt 'Step9GlueRole.Arn'
      Description: AWS Glue crawler to crawl cloudfront accesslog
      DatabaseName: !Ref 'Step9GlueDatabase'
      Targets:
        S3Targets:
          - Path: !Sub 's3://${Step8S3Bucket}/Step7/'
      TablePrefix: cloudfront
      SchemaChangePolicy:
        UpdateBehavior: UPDATE_IN_DATABASE
        DeleteBehavior: LOG

  Step10ScheduledRule:
    Type: AWS::Events::Rule
    Properties:
      Description: ScheduledRule
      ScheduleExpression: cron(40 0 * * ? *)
      State: ENABLED
      Targets:
        - Arn: !GetAtt 'Step11LambdaFunction.Arn'
          Id: Step11LambdaFunction
  Step11LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref 'Step11LambdaFunction'
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt 'Step10ScheduledRule.Arn'
  Step11LogGroupLambda:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${AWS::StackName}/${Step11LambdaFunction}'
      RetentionInDays: 7
  Step11LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:*
                Resource: !Sub 'arn:aws:s3:::${AWS::StackName}-*'
              - Effect: Allow
                Action:
                  - athena:*
                  - glue:Get*
                Resource: '*'

  Step11LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt Step11LambdaRole.Arn
      Code:
        ZipFile: !Sub |
          import boto3
          import os
          def lambda_handler(event, context):
            database = os.environ['database']
            outputlocation = os.environ['outputlocation']
            client = boto3.client('athena')
            sql = 'MSCK REPAIR TABLE ${AWS::StackName}step7;'
            client.start_query_execution(
              QueryString=sql,
              QueryExecutionContext={'Database': database},
              ResultConfiguration={'OutputLocation': outputlocation}
            )
      Runtime: python3.6
      MemorySize: 128
      Timeout: 180
      Description: Update Athena partition (MSCK REPAIR TABLE)
      Environment:
        Variables:
          database: cloudfront
          outputlocation: !Sub 's3://${Step8S3Bucket}/Step11/'
      Tags:
        - Key: CloudformationArn
          Value: !Ref 'AWS::StackId'

Outputs:
  S3BucketSource:
    Value: !Ref 'Step1S3Bucket'
  S3BucketJson:
    Value: !Ref 'Step5S3Bucket'
  S3BucketAthena:
    Value: !Ref 'Step8S3Bucket'

